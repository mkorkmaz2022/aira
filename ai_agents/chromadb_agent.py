# # ai_agents/chromadb_agent.py
# import os
# from langchain_community.document_loaders import TextLoader # DoÄŸru import
# from langchain_text_splitters import RecursiveCharacterTextSplitter # DoÄŸru import
# from langchain_community.vectorstores import Chroma
# from langchain_google_genai import GoogleGenerativeAIEmbeddings # DoÄŸru import
# from dotenv import load_dotenv
# from ai.google_ai import GoogleAIClient # Mevcut LLM istemciniz

# load_dotenv()

# # Sabitler
# CHROMA_DB_PATH = "./chroma_db"
# EMBEDDING_MODEL = "models/embedding-001" 

# def create_and_store_notes_db(meeting_notes: str) -> Chroma | str:
#     """
#     ToplantÄ± notlarÄ±nÄ± alÄ±r, parÃ§alara ayÄ±rÄ±r, vektÃ¶rleÅŸtirir ve ChromaDB'ye kaydeder (SCUM-109).
#     """
#     if not meeting_notes or not meeting_notes.strip():
#         return "HATA: VektÃ¶rleÅŸtirilecek notlar boÅŸ."
    
#     # GeÃ§ici dosyaya yazma (LangChain loader iÃ§in)
#     temp_file_path = "temp_notes.txt"
#     with open(temp_file_path, "w", encoding="utf-8") as f:
#         f.write(meeting_notes)

#     try:
#         # 1. Belge YÃ¼kleme ve ParÃ§alama (Chunking)
#         loader = TextLoader(temp_file_path, encoding="utf-8")
#         documents = loader.load()

#         text_splitter = RecursiveCharacterTextSplitter(
#             chunk_size=1000,
#             chunk_overlap=200
#         )
#         docs = text_splitter.split_documents(documents)
        
#         # 2. Embeddings (GÃ¶mme) Modelini HazÄ±rla ve API AnahtarÄ±nÄ± Ä°let
#         google_api_key = os.getenv("GOOGLE_AI_API_KEY") 
#         if not google_api_key:
#             return "API_HATA: GOOGLE_AI_API_KEY bulunamadÄ±. LÃ¼tfen .env dosyasÄ±nÄ± kontrol edin."

#         # ğŸš¨ KRÄ°TÄ°K DÃœZELTME: LangChain'in aradÄ±ÄŸÄ± anahtar adÄ±nÄ± (GOOGLE_API_KEY) ayarlayÄ±n
#         os.environ["GOOGLE_AI_API_KEY"] = google_api_key
            
#         embeddings = GoogleGenerativeAIEmbeddings(
#             model=EMBEDDING_MODEL,
#             api_key=google_api_key # Parametre olarak da iletilmeye devam et
#         )
        
#         # 3. ChromaDB'ye Ekleme ve Ä°ndeksleme
#         print(f"âœ… Notlar parÃ§alara ayrÄ±ldÄ± ({len(docs)} chunk). ChromaDB'ye ekleniyor...")
        
#         vectordb = Chroma.from_documents(
#             documents=docs, 
#             embedding=embeddings, 
#             persist_directory=CHROMA_DB_PATH
#         )
        
#         # KaydÄ± tamamla ve dosyayÄ± temizle
#         vectordb.persist()
#         os.remove(temp_file_path)
        
#         print(f"âœ… ChromaDB oluÅŸturuldu ve {CHROMA_DB_PATH} dizinine kaydedildi.")
#         return vectordb

#     except Exception as e:
#         if os.path.exists(temp_file_path):
#             os.remove(temp_file_path)
#         return f"API_HATA: ChromaDB oluÅŸturulurken hata oluÅŸtu: {e}"

# def retrieve_and_summarize_notes(query: str, vectordb: Chroma) -> str:
#     """
#     Sorguya en yakÄ±n not parÃ§alarÄ±nÄ± ChromaDB'den getirir ve bu baÄŸlamla AI'dan Ã¶zet ister (SCUM-110).
#     """
    
#     # 1. Anlamsal Arama (Retrieval)
#     retrieved_docs = vectordb.similarity_search(query, k=4)
    
#     # 2. BaÄŸlamÄ± (Context) OluÅŸtur
#     context_text = "\n\n---\n\n".join([doc.page_content for doc in retrieved_docs])
    
#     # 3. RAG Ä°stemi (Prompt) OluÅŸtur
#     prompt = f"""
#     Sen bir ToplantÄ± Ã–zeti UzmanÄ±sÄ±n. Sana sunulan "BAÄLAM" metinlerini kullanarak, aÅŸaÄŸÄ±daki "SORGU"ya en uygun ve kÄ±sa Ã¶zeti Ã§Ä±kar. YanÄ±tÄ±nÄ± kesinlikle sadece sunulan BAÄLAM'daki bilgilere dayanarak oluÅŸtur. Teknik jargon kullanma ve karar vericiye hitap et.

#     BAÄLAM (ChromaDB'den Gelen Ä°lgili Not ParÃ§alarÄ±):
#     ---
#     {context_text}
#     ---

#     SORGU: {query}
    
#     Ã‡IKTI FORMATI:
#     **ToplantÄ± Ã–zeti** (KÄ±sa, net ve taranabilir Ã¶zet - 2-4 cÃ¼mle)
#     **GerekÃ§e**: (YanÄ±tÄ±nÄ± neden bu baÄŸlama dayanarak verdiÄŸini belirten 1 cÃ¼mle)

#     BaÅŸla:
#     """
    
#     # 4. LLM'e GÃ¶nder (Generation)
#     try:
#         # GoogleAIClient, API key'i kendi iÃ§inde okur.
#         client = GoogleAIClient()
#         response = client.send_message(prompt) 
#         return response
#     except Exception as e:
#         return f"API_HATA: Mesaj gÃ¶nderilirken hata oluÅŸtu: {e}"
# ai_agents/chromadb_agent.py
import os
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from dotenv import load_dotenv
from ai.google_ai import GoogleAIClient

load_dotenv()

CHROMA_DB_PATH = "./chroma_db"

# HuggingFace embedding modeli
HF_EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"

def create_and_store_notes_db(meeting_notes: str) -> Chroma | str:

    if not meeting_notes or not meeting_notes.strip():
        return "HATA: VektÃ¶rleÅŸtirilecek notlar boÅŸ."
    
    temp_file_path = "temp_notes.txt"
    with open(temp_file_path, "w", encoding="utf-8") as f:
        f.write(meeting_notes)

    try:
        # 1. Belge YÃ¼kleme ve Chunking
        loader = TextLoader(temp_file_path, encoding="utf-8")
        documents = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        docs = text_splitter.split_documents(documents)

        # ---------------------------------------------
        # ğŸ”¥ HuggingFace Embeddings KullanÄ±mÄ±
        # ---------------------------------------------
        embeddings = HuggingFaceEmbeddings(
            model_name=HF_EMBEDDING_MODEL
        )
        # ---------------------------------------------

        print(f"âœ… Notlar parÃ§alara ayrÄ±ldÄ± ({len(docs)} chunk). ChromaDB'ye ekleniyor...")

        vectordb = Chroma.from_documents(
            documents=docs,
            embedding=embeddings,
            persist_directory=CHROMA_DB_PATH
        )

        vectordb.persist()
        os.remove(temp_file_path)

        print(f"âœ… ChromaDB oluÅŸturuldu ve {CHROMA_DB_PATH} dizinine kaydedildi.")
        return vectordb

    except Exception as e:
        if os.path.exists(temp_file_path):
            os.remove(temp_file_path)
        return f"API_HATA: ChromaDB oluÅŸturulurken hata oluÅŸtu: {e}"
    
def retrieve_and_summarize_notes(query: str, vectordb: Chroma) -> str:
    """
    Sorguya en yakÄ±n not parÃ§alarÄ±nÄ± ChromaDB'den getirir ve bu baÄŸlamla AI'dan Ã¶zet ister (SCUM-110).
    """
    
    # 1. Anlamsal Arama (Retrieval)
    retrieved_docs = vectordb.similarity_search(query, k=4)
    
    # 2. BaÄŸlamÄ± (Context) OluÅŸtur
    context_text = "\n\n---\n\n".join([doc.page_content for doc in retrieved_docs])
    
    # 3. RAG Ä°stemi (Prompt) OluÅŸtur
    prompt = f"""
    Sen bir ToplantÄ± Ã–zeti UzmanÄ±sÄ±n. Sana sunulan "BAÄLAM" metinlerini kullanarak, aÅŸaÄŸÄ±daki "SORGU"ya en uygun ve kÄ±sa Ã¶zeti Ã§Ä±kar. YanÄ±tÄ±nÄ± kesinlikle sadece sunulan BAÄLAM'daki bilgilere dayanarak oluÅŸtur. Teknik jargon kullanma ve karar vericiye hitap et.

    BAÄLAM (ChromaDB'den Gelen Ä°lgili Not ParÃ§alarÄ±):
    ---
    {context_text}
    ---

    SORGU: {query}
    
    Ã‡IKTI FORMATI:
    **ToplantÄ± Ã–zeti** (KÄ±sa, net ve taranabilir Ã¶zet - 2-4 cÃ¼mle)
    **GerekÃ§e**: (YanÄ±tÄ±nÄ± neden bu baÄŸlama dayanarak verdiÄŸini belirten 1 cÃ¼mle)

    BaÅŸla:
    """
    
    # 4. LLM'e GÃ¶nder (Generation)
    try:
        # GoogleAIClient, API key'i kendi iÃ§inde okur.
        client = GoogleAIClient()
        response = client.send_message(prompt) 
        return response
    except Exception as e:
        return f"API_HATA: Mesaj gÃ¶nderilirken hata oluÅŸtu: {e}"
